{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final model (Git).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8K1POSf4Iw6",
        "colab_type": "text"
      },
      "source": [
        "# import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz6Q5Sj63NlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os \n",
        "import gzip\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa\n",
        "\n",
        "from datetime import datetime \n",
        "\n",
        "from scipy import signal\n",
        "from scipy.signal import spectrogram\n",
        "\n",
        "#! pip install lspopt\n",
        "#from lspopt.lsp import spectrogram_lspopt\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization, concatenate\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Permute, Reshape\n",
        "from keras.layers import Convolution1D, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils, plot_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_btcP7Rv4M63",
        "colab_type": "text"
      },
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWinF8LO3lhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def model():\n",
        "    # Input A = Raw, Input B = Spectogram\n",
        "    inputA = Input(shape=(3000,1))\n",
        "    inputB = Input(shape=(2, 129, 13))\n",
        "\n",
        "    # the first branch operates on the first input Raw\n",
        "    raws = BatchNormalization()(inputA)\n",
        "    raws = Conv1D(filters=60, kernel_size=20, activation='relu')(raws)\n",
        "    raws = MaxPooling1D(pool_size=6)(raws)\n",
        "    raws = Conv1D(filters=80, kernel_size= 15, activation='relu')(raws)\n",
        "    raws = MaxPooling1D(pool_size=6)(raws)\n",
        "    raws = Dropout(0.2)(raws)\n",
        "    raws = Conv1D(filters=80, kernel_size= 10, activation='relu')(raws)\n",
        "    raws = MaxPooling1D(pool_size=6)(raws)\n",
        "    raws = Conv1D(filters=80, kernel_size= 10, activation='relu')(raws)\n",
        "    raws = MaxPooling1D(pool_size=2)(raws)\n",
        "    raws = Dense(120, activation='relu')(raws)\n",
        "    raws = GlobalAveragePooling1D()(raws)\n",
        "\n",
        "    raws = Model(inputs = inputA, outputs = raws)\n",
        "\n",
        "    # the second branch opreates on the second input\n",
        "    spect = BatchNormalization()(inputB)\n",
        "    spect = Conv2D(filters=64, kernel_size = (1,6), activation='relu')(spect)\n",
        "    spect = MaxPooling2D(pool_size=(1,2))(spect)\n",
        "    spect = Dropout(0.2)(spect)\n",
        "\n",
        "    spect = BatchNormalization()(spect)\n",
        "    spect = Conv2D(filters=128, kernel_size=(1,4), activation='relu')(spect)\n",
        "    spect = MaxPooling2D(pool_size=(1,2))(spect)\n",
        "    spect = Dropout(0.2)(spect)\n",
        "\n",
        "    spect = BatchNormalization()(spect)\n",
        "    spect = Conv2D(filters=256, kernel_size=(1,2), activation='relu')(spect)\n",
        "    spect = MaxPooling2D(pool_size=(1,2))(spect)\n",
        "    spect = Dropout(0.2)(spect)\n",
        "\n",
        "    spect = BatchNormalization()(spect)\n",
        "    spect = Conv2D(filters=64, kernel_size=(1,2), activation='relu')(spect)\n",
        "    spect = MaxPooling2D(pool_size=(1,2))(spect)\n",
        "    spect = Dropout(0.2)(spect)\n",
        "\n",
        "    spect = GlobalAveragePooling2D()(spect)\n",
        "\n",
        "    spect = Model(inputs = inputB, outputs = spect)\n",
        "\n",
        "    # combine the output of the two branches\n",
        "    combined = concatenate([raws.output, spect.output])\n",
        "\n",
        "    # apply a FC layer and then a regression prediction on the combined outputs\n",
        "    z = Dense(6, activation='softmax')(combined)\n",
        "\n",
        "    # our model will accept the inputs of the two branches and outputs a vector\n",
        "    model = Model(inputs=[raws.input, spect.input], outputs=z)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqK_twc04WX9",
        "colab_type": "text"
      },
      "source": [
        "# Compile model and fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huuf8gGAXuuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading Data\n",
        "with open('/src/Data_Raw_signals.pkl', 'rb') as raw:\n",
        "  train_data_raw, class_raw = pickle.load(raw)\n",
        "\n",
        "def multi_model(batch, window_size, order, patience, num_epochs, c,\n",
        "                train_data_raw = train_data_raw, model = model()):\n",
        "\n",
        "  #Apply Savgol filter to raw signals\n",
        "  train_data_raw_sav = savgol_filter(train_data_raw[:,0,:], window_size, order).reshape(-1,3000,1)\n",
        "\n",
        "  # Getting Spectograms, and demonstrating the results\n",
        "  # Change c_perameter to change resolution. lower = more resolute\n",
        "  f, t, Sxx = spectrogram(train_data_raw, fs=100)\n",
        "  Sxx1 = librosa.power_to_db(Sxx, ref = np.mean)\n",
        "\n",
        "  #Split the data\n",
        "  raw_train, raw_val, y_train, y_val = train_test_split(train_data_raw_sav, class_raw, test_size=0.2, random_state = 42)\n",
        "  spect_train, spect_val, y_train, y_val = train_test_split(Sxx1, class_raw, test_size=0.2, random_state = 42)\n",
        "\n",
        "  # Compile the model\n",
        "  #optim = optimizers.SGD(momentum=0.9, nesterov=True)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', \n",
        "                metrics=['sparse_categorical_accuracy'], \n",
        "                optimizer='adam')\n",
        "\n",
        "  #Fitting model\n",
        "  num_epochs = num_epochs\n",
        "  num_batch_size = batch\n",
        "\n",
        "  checkpointer = [ModelCheckpoint(filepath= \"/out/mixed_mod_simplesect_1draw_otherchannel.hdf5\", \n",
        "                                verbose=1, save_best_only=True),\n",
        "                  EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = patience)]\n",
        "\n",
        "  start = datetime.now()\n",
        "\n",
        "  history = model.fit([raw_train, spect_train], y_train, \n",
        "            batch_size=num_batch_size, \n",
        "            epochs=num_epochs, \n",
        "            validation_data=([raw_val, spect_val], y_val), \n",
        "            callbacks=checkpointer, \n",
        "            verbose=1)\n",
        "\n",
        "  duration = datetime.now() - start\n",
        "  print(\"Training completed in time: \", duration)\n",
        "\n",
        "  # Obtaining accuracy  \n",
        "  model.load_weights(\"/out/mixed_mod_simplesect_1draw_otherchannel.hdf5\")\n",
        "  _, val_acc = model.evaluate([raw_val, spect_val], y_val)\n",
        "\n",
        "  return val_acc, raw_val, spect_val, y_val, model\n",
        "\n",
        "val_acc, raw_val, spect_val, y_val, model = multi_model(12, 13, 4, c = 2.5, \n",
        "                                                        num_epochs = 200, \n",
        "                                                        patience = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UW6x5Az4B9d",
        "colab_type": "text"
      },
      "source": [
        "# Perform prediction and obtain metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvyKv2FNXuqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start Here\n",
        "# Getting metrics\n",
        "model.load_weights(\"/out/mixed_mod_simplesect_1draw_otherchannel.hdf5\")\n",
        "\n",
        "y_pred = model.predict([raw_val, spect_val], batch_size= 1, verbose=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_true = y_val\n",
        "\n",
        "classes_labels = ['class R', 'class 1', 'class 2', 'class 3', 'class 4', 'class W']\n",
        "\n",
        "np.set_printoptions(precision=2, suppress='true')\n",
        "cm_norm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "\n",
        "# Plot of normalized confusion matrix \n",
        "df_cm_norm = pd.DataFrame(cm_norm, classes_labels, classes_labels)\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm_norm, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "plt.savefig('/out/Confusion_matrix.png')\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=classes_labels))\n",
        "\n",
        "# Tο ΤΧΤ\n",
        "# Reading test data\n",
        "with open('/src/Test_Raw_signals_no_labels.pkl', 'rb') as test:\n",
        "  test_data_raw = pickle.load(test)\n",
        "\n",
        "test_data_raw = test_data_raw[0]\n",
        "\n",
        "# Filtering test data\n",
        "test_data_raw_sav = savgol_filter(test_data_raw[:,0,:], 13, 4).reshape(-1, 3000, 1)\n",
        "\n",
        "#Exporting Spectograms from raw data\n",
        "f, t, Spect_test = spectrogram(test_data_raw, fs=100)\n",
        "spect_test = librosa.power_to_db(Spect_test, ref = np.mean)\n",
        "\n",
        "model.load_weights(\"/out/mixed_mod_simplesect_1draw_otherchannel.hdf5\")\n",
        "\n",
        "prediction_result = model.predict([test_data_raw_sav, spect_test], batch_size= 12, verbose=1)\n",
        "prediction_result_arg = np.argmax(prediction_result, axis = 1)\n",
        "prediction_label = pd.DataFrame(prediction_result_arg, columns=[\"Stage\"])\n",
        "\n",
        "test = pd.DataFrame(test_data_raw_sav[:, 0, 0])\n",
        "final = pd.concat([test, prediction_label], axis = 1) #1\n",
        "final.to_csv('/out/answer.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}